![postgres!](https://github.com/mochen862/data-engineering-projects/blob/main/data-modelling-with-postgres/postgres.png)

# Purpose of the Sparkify's database and analytical goals

- Sparkify, a startup, wants to analyse data collected on songs and user activity on the new music streaming app, with a particular focus on what songs the users are listening to
- Currently the data resides in a directory of JSON logs on user activity on the app, and a directory with JSON metadata on songs in the app (i.e. there's no easy way to query the data)
- The purpose here is to create a Postgres database to optimise queries on song play analysis, and build an ETL pipeline in Python. To do this, I created fact and dimension tables for a star schema, wrote an ETL pipeline that transferred data from files in two local directories into Postgres tables using Python and SQL

# How to run the Python scripts (see the `run_script` Jupyter notebook)

1. Run `create_tables.py` to create the sparkify database, and the fact and dimension tables
2. Run `etl.py` to insert data into the tables

# Explanation of the files in the repository

## Song Data
- this is a subset of real data from the [Million Songs Dataset](http://millionsongdataset.com/)
- each file is in JSON format and contains metadata about a song and the artist of the song
- the files are partitioned by the first three letters of each song's track ID. For example, here's a filepath to a file in the dataset
    - `song_data/A/B/C/TRABCEI128F424C983.json`
- and an example of what a single song file looks like

`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

## Log Data
- this dataset contains log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above
- These simulate activity logs from a music streaming app based on specified configurations.
- The log files in the dataset you'll be working with are partitioned by year and month. For example, here's a filepath to a file
    - `log_data/2018/11/2018-11-12-events.json`
- And here's what the log data looks like

![log data!](https://github.com/mochen862/data-engineering-projects/blob/main/data-modelling-with-postgres/log_data_df.PNG)
        
# Database schema design and ETL pipeline

![star schema!](https://github.com/mochen862/data-engineering-projects/blob/main/data-modelling-with-postgres/star_schema.png)

## Star schema

- the database follows a star schema, with `songplays` as the fact table and `users, songs, artists,` and `time` as the four dimension tables
- the chosen schema is the star schema as the particular focus of the Sparkify analytics team is around the ability to query what songs the users are listening to
- the `songplays` table records in log data associated with song plays (i.e. records with page NextSong) and contains the below fields
    - *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*
- the `users` table records users in the app and contains the below fields
    - *user_id, first_name, last_name, gender, level*
- the `songs` table records songs in the music database and contains the below fields
    - *song_id, title, artist_id, year, duration*
- the `artists` table records the artists in the music databse and contains the below fields
    - *artist_id, name, location, latitude, longitude*
- the `time` table records the timestamps of records in `songplays` broken down into specific units and contains the below fields
    - *start_time, hour, day, week, month, year, weekday*
    
## ETL pipeline

- the ETL pipeline is broken down to two Python files; `create_tables.py` and `etl.py`
- `create_tables.py` creates the database and the tables, contains the insert statements into the tables and the statement that will allow us to match the song title, the artist name and the song duration in the `songs` and `artists` tables with song name, artist name and song length in the `songplays` table
- `etl.py` loads the song_data and log_data, inserts the records into the 5 tables, and finds the matching song id and artist id in the `songs` and `artists` table to include in the `songplays` table

# Songplay analysis

- There is one matching record based on song title, artist name and song duration between the **songs** and **artists** tables and the **songplays** table
- See sample queries in the `run_script` Jupyter notebook

